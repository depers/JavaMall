filebeat.inputs:
- type: log
  paths:
      - /usr/local/myapp/collector/logs/app-collector.log
  docutment_type: "app-log"  # 定义写入ES时的 _type值
  multiline:
    pattern: '^\['  # 指定匹配的表达式（匹配以 [ 开头的字符串）
    negate: true     # 是否匹配到
    match: after     # 不匹配的合并到上一行的末尾
    max_lines: 2000  # 合并的最大行数
    timeout: 2s      # 如果在规定时间没有新的日志事件就不等待后面的日志
  fields:
    logbiz: collector
    logtopic: app-log-collector  # 按服务划分用作kafka topic
    env: dev

- type: log
  paths:
    - /usr/local/myapp/collector/logs/error-collector.log
  docutment_type: "error-log"  # 定义写入ES时的 _type值
  multiline:
    pattern: '^\['  # 指定匹配的表达式（匹配以 [ 开头的字符串）
    negate: true     # 是否匹配到
    match: after     # 不匹配的合并到上一行的末尾
    max_lines: 2000
    timeout: 2s
  fields:
    logbiz: collector
    logtopic: error-log-collector
    env: dev

output.kafka:
  enabled: true
  hosts: ["192.168.156.135:9092"]
  topic: '%{[fields.logtopic]}'
  partition.hash:
    reachable_only: true
  compression: gzip
  max_message_bytes: 1000000
  required_acks: 1
logging.to_files: true